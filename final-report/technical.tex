% Force column break to prevent section header from being in previous column
\vfill\eject

\section{Technical Details}
\label{sec:technical}

Guide Dog is composed of the three separate components described below: destination detection
(see Section~\ref{sec:technical-dest}),
obstacle detection (see Section~\ref{sec:technical-obs}), and the audio
interface (see Section~\ref{sec:technical-audio}).

\begin{figure}
\includeimage{color.pdf}{8cm}{bb=0 0 474 474}
\caption{A sample scene, viewed in color. The destination object is the pink box
  in the middle. The two stacks of brown boxes on the left and right are
  obstacles.}
\label{fig:color}
\end{figure}

\subsection{Destination Detection}
\label{sec:technical-dest}

\begin{figure}
\includeimage{destination.pdf}{8cm}{bb=0 0 474 474}
\caption{A similarity view of Figure~\ref{fig:color} showing regions of the scene with a high similarity
to the color of the destination object. The green dot indicates the predicted location of the destination.}
\label{fig:destination}
\end{figure}

\subsubsection{Overview}
\label{sec:technical-dest-overview}

Guide Dog's destination detection component is responsible for identifying the
destination object in the scene, and determining its position in space
relative to the camera. Guide Dog was originally imagined as a system that
would be able to detect a wide variety of objects, but we were forced to
build a simpler prototype due to time constraints. As a result, the component 
is only able to detect objects of a solid color that contrast highly from the 
scene. Functionality is provided for re-calibrating the system for a new color, 
or for different lighting or exposure conditions. 

\subsubsection{Operation}
\label{sec:technical-dest-op}

To set up the destination detection component the user must first perform a
calibration step in which the color of the destination object is measured.
This measurement can be performed on the fly, by placing the
destination object in front of the camera, and then measuring the average 
color within the center region of the image. Once the color of the destination
object is known, we compute a similarity image where each pixel represents the 
similarity of an input pixel's color to the measured destination object color. Once 
this image is obtained, we binarize the pixel values by applying a 
user-adjustable threshold. The result is a binary image like
Figure~\ref{fig:destination}. A \emph{blob detection} algorithm is then run over the 
binary image, which finds the largest connected components by treating it as an 
8-connected graph. If multiple blobs are detected, the largest one is used.

Since the blobs describe a pixel location in the image, we must now convert 
this location into a real-world position relative to the camera. We take
advantage of PCL~\cite{pcl-website}, which provides camera-relative XYZ coordinates
for each pixel. We use this to compute the average XYZ position for pixels
within the largest blob, which then is returned as the final estimate of the location
of the destination object. 


\subsubsection{Implementation Details}
\label{sec:technical-dest-impl}

The similarity metric we use to compute the binary image in Figure~\ref{fig:destination}
treats all pixels as vectors in 3-space. In order
to compute these similarity values, we first normalize all pixel vectors
to have unit length. We then represent the difference between two pixels using the
magnitude of their difference vector. For example, if \emph{c} is the color of a
given pixel, and \emph{d} is the color of the destination object, then this gives the 
difference \begin{math} \textit{diff} = c - d \end{math} and the similarity 
\begin{math}\textit{sim} = 1 - ||\textit{diff}|| \end{math}.

In order to make the system more robust to noise, we apply a Gaussian blur
filter to the similarity image. We found a filter kernel size of 10 to 15 to work 
best for our application. Since the similarity computation tends
to amplify the appearance of noise, and also because noise tends to
negatively effect blob detection, this added step helps significantly increase
the robustness of the component.

After computing a similarity image in this fashion, we binarize the image
using a simple user-set threshold, and pass the binary image to the OpenCV~\cite{opencv-website}
\emph{SimpleBlobDetector}. This routine reliably detects connected components
within the image, but requires many parameters to be set properly, such as
minimum and maximum  blob size, convexity, and separation. After detecting blobs, the 
largest blob is used to compute a centroid in 3-space using pixels belonging 
to the blob.

\subsection{Obstacle Detection}
\label{sec:technical-obs}

\begin{figure}
\includeimage{obstacle.pdf}{5cm}{bb=0 0 300 300}
\caption{A view of the scene in Figure~\ref{fig:color}. This is a top down view.
  The user's location is shown with the blue X. The obstacles are shown labeled
  with red dots and the destination is labeled with a green dot.}
\label{fig:obstacle}
\end{figure}

\subsubsection{Overview}
\label{sec:technical-obs-overview}

The second component of Guide Dog is the component to detect obstacles. This
component must detect the obstacles near the user and convey the obstacles'
locations to the audio interface. Only obstacles near the user are detected in
order to avoid overwhelming the user. It is only important for the user to
know if he or she is about to run into an obstacle, not if there is obstacle on the
other side of the room.

\subsubsection{Operation}
\label{sec:technical-obs-op}

The obstacle detection algorithm looks only at the depth information from the
RGB-D camera. From this, it can see objects such as the floor, obstacles and
walls. Intuitively, all of the obstacles will be above the floor, while the floor
itself is not an obstacle. This leads to a simple algorithm to detect the
obstacles: just remove the floor and everything left is an obstacle.

\subsubsection{Implementation Details}
\label{sec:technical-obs-impl}

The obstacle detection algorithm views the points in a format created by the
Point Cloud Library (PCL)~\cite{pcl-website}. Each individual pixel viewed by
the camera is represented by an X coordinate, Y coordinate, depth and RGB color.
A data structure storing all of these points is called a ``point cloud'' and
represents the whole 3D environment that the camera can see.

The obstacle detection algorithm first looks at the point cloud and extracts the
plane of the floor. PCL has a built-in planar segmentation library function that
detects the largest plane in view (there is an issue if the floor is not the
largest plane in view, see Section~\ref{sec:eval-obs}).
The plane detection algorithm gives the \emph{a}, \emph{b}, \emph{c}, and
\emph{d} coefficients in the following planar equation:

\begin{math}
ax + by + cz + d = 0
\end{math}

This results in an equation representing the plane of the floor. The obstacle
detection algorithm then removes all points that lie within a certain threshold
of the floor plane. The threshold helps remove noise added by the camera and is
currently set to 10 centimeters. After removing the floor, only the obstacles
are left. However, the obstacles are represented in 3D space, which is not
necessary for obstacle detection. This is because the height of an obstacle does
not matter. If there is an obstacle at any height, this needs to be
communicated to the user. This allows the obstacle detection algorithm to
convert the 3D obstacles to 2D space. To do this, the 3D obstacle coordinates
are projected onto the floor plane. Then, the floor plane is rotated so that it
is level. This provides a simple way to analyze the obstacles and calculate
distances.

Once the obstacles have been extracted and projected into a 2D space, more
analysis is performed to detect their locations. The obstacles are processed
into a black and white image: a white pixel means there is part of an obstacle
at that location, a black pixel means that location is empty. An example of this
image is shown in Figure~\ref{fig:obstacle}. This image is then
passed off to the same blob detection algorithm described in Section~\ref{sec:technical-dest-impl}.
As is shown in the example image in
Figure~\ref{fig:obstacle}, each obstacle is represented as a contiguous block
of white pixels. The blob detection algorithm detects the contiguous blocks and
produces a single point for each blob, marked with a red dot in the example
figure.

At this point, the obstacle detection algorithm has the locations of all
obstacles. There are two more steps that must happen before the obstacles are
sent to the audio interface. First, the obstacle detection component must
communicate with the destination detection component. This is because
the obstacle detection component has no idea where the destination object is,
meaning the destination component will get detected and marked as an obstacle!
In order to prevent this, the obstacle detection component gets the coordinates
of the destination object from the destination detection component. It then
compares the coordinates of each obstacle it found with the coordinates of the
destination object. If any of the obstacle coordinates are close enough to the
destination, the obstacle detection component assumes that the obstacle must
actually be the destination and removes it from its list of obstacles. Second,
the obstacle detection component is only supposed to communicate obstacles that
are near to the user, not all obstacles. To do this, the obstacle detection
component simply removes any obstacles that are too far away from the user.

At this point, the remaining obstacles are ready to be passed off to the audio
interface so they can be communicated to the user.

\subsection{Audio Interface}
\label{sec:technical-audio}

\subsubsection{Overview}
\label{sec:technical-audio-overview}

%(Provide the context of what your system does. A drawing might be useful here.)
Because the intended user of Guide Dog may not have normal vision, the audio 
component is meant to provide guidance to the user without the need of sight. It
uses 3D audio cues to direct the user toward the destination and warn of any 
obstacles in the user's way. This means that the audio cues appear as though
they emanate from a particular point in 3D space that corresponds to the
destination or obstacle.

\subsubsection{Operation}
\label{sec:technical-audio-op}

%(Detailed description of the different functionalities and how they work. For
%instance, describe your tracking / smoothing / recognition algorithm.)
The audio system signals the location of the destination through the use of an
audio beacon that sounds like a series of synthesized echoing beeps. It is
fairly easy to distinguish changes in tone using the beeps and it is not a sound
that quickly gets annoying to the user. The user is expected to follow the
direction of the beacon in order to get to the destination. To communicate
distance to the destination, the audio system increases or decreases the pitch
of the beacon as the user gets closer or further away, respectively. Once the
user arrives within a few feet from the destination, the system plays a jingle
to tell the user that he or she has arrived. 

As for obstacles, the audio system uses a ding that sounds similar to the one
that is played when a car door is left ajar. Like with the beacon, the goal was
to find a sound that was pleasant over moderate periods of time.  Furthermore,
the ding is clearly distinguishable from the beacon. Unlike the destination
beacon, the user is expected to avoid the direction of the alert.

The audio system defines five discrete regions that an object may be located
within. Compared to playing the sound purely as though it were emanating from
its actual location in 3D space, these regions help the system exaggerate the
left and right directions when sounds are played back to the user.  This
strategy gives a clearer indication of which direction an object is located in.
These regions are based on the angle of the object's location relative to the
user. The five regions are defined as full left, partial left, in front, partial
right and full right. Section~\ref{sec:eval-audio} describes the layout and
development of the region system in greater detail.

\subsubsection{Implementation Details}
\label{sec:technical-audio-impl}

%(Some details on the software implementation.)
The audio system is abstracted away from the other parts of Guide Dog and
written as a class with member functions that the other systems can invoke. At 
the core of this class, known as SonicDog, is OpenAL~\cite{openal-website}, which
is an open source audio library typically used in video games to handle 3D
sound. Developing a separate class enabled quick development of the audio system
without affecting other parts of GuideDog because the clients only needed to be
concerned with inputting the correct and consistent coordinates of objects in 
the scene.

SonicDog has a thread pool and job queue for the threads. The destination
and obstacle systems fill the queue with sound sources and the threads pop them 
off and play a sound for each source. OpenAL uses the concepts of a listener, to 
define the coordinates of the person listening, and sources, to define the 
objects playing a sound. The listener and sources both have locations within the 
OpenAL world. SonicDog defines the listener at \emph{(0, 0, 0)} under the 
\emph{(x, y, z)} coordinate system. All other sound sources are placed relative 
to the listener by defining the \emph{x} and \emph{z} coordinates of the source 
since the other systems in Guide Dog reduce the world to a 2D plane. Positive 
\emph{x} denotes an object being to the right of the listener and negative 
\emph{x} as being to the left. Similarly, positive \emph{z} is in front and 
negative \emph{z} is behind. Furthermore, each source has a sound buffer, which 
defines the sound to be played for the source.

To signal destinations, the destination system registers the object with
SonicDog, which initializes the source and returns an identification number. 
The destination system then refers to this number when it wants to update the
new location of the registered object. Each time Guide Dog updates the location,
there is a subroutine in SonicDog that calculates the angle of the object 
relative to the user and then manipulates the internal OpenAL coordinates to 
place it in the correct region. Meanwhile, the thread responsible for playing 
the beacon continuously plays the beacon every 1.5 seconds and then 
recalculates the pitch of the beacon based on the current values it has for the 
position of the destination object. SonicDog handles obstacles differently.

Because obstacles are not tracked from frame to frame, SonicDog provides an
interface to alert a group of obstacles once. Each time the obstacle detection
system runs, it can pass in a list of obstacle locations to SonicDog. The same
subroutine runs to place the obstacles into their respective regions, a new
source is created in OpenAL, and then the source is pushed onto the job queue.
From there, a worker thread pulls them off one by one and tells OpenAL to play
the source. Afterwards, the thread cleans up all the memory allocated for the
source and goes back to pull more sources off the job queue or blocks if the 
queue is empty.

The math used to place an object in a specific region is straightforward. Given
the \emph{x} and \emph{z} coordinates of an object, SonicDog uses arctangent to
calculate the angle \begin{math}\theta\end{math} between the object and the
listener, and the Pythagorean theorem to calculate the distance \emph{d} from
the object to the listener. In the five-region design in Figure~\ref{fig:regions}, the
middle region is the arc considered in front of the user and is between 85 and
95 degrees. When \begin{math}85\leq\theta\leq95\end{math}, SonicDog sets the
internal coordinates to \emph{(0, 0, d)} so that OpenAL plays a sound that
appears as though it is coming from in front of the user. If
\begin{math}\theta>110\end{math}, SonicDog places the source at \emph{(-d, 0,
0)} because that region is considered the complete left. A similar operation
occurs for when \begin{math}\theta<70\end{math} and the object is on the
complete right. For the 15 degree arcs representing the partial left or right
regions, SonicDog uses the equation \begin{math}\phi=2\theta-140\end{math} to
translate the source to an angle \begin{math}\phi\end{math} in the OpenAL world
that is further left or right than the real world location. This makes the
changes in stereo more audible to the user. The effect is that the user
hears an emphasis of the sound in one ear instead of it being completely pushed
to that ear. This tells the user that the object is only located slightly to the
left or right instead it being ambiguous like in the three region design. The
corresponding internal coordinates would be set to \begin{math}(d\cos\phi, 0,
d\sin\phi)\end{math}. Section \ref{sec:eval-audio} goes into further details
about the regions used by SonicDog.

Since this is a multi-threaded system, the implementation required multiple
locks to synchronize access to the maps inside SonicDog that keep track of what
objects have been registered to it. In addition, condition variables are used to
synchronize the job queue. These allow for a thread to be blocked and not use 
system resources when the queue is empty and for broadcasts to wake up threads 
when the queue becomes non-empty.
