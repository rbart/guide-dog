\section{Technical Details (\textasciitilde 2-3 pages)}
\label{sec:technical}

Guide Dog is composed of the three separate components described below: destination detection
(see Section~\ref{sec:technical-dest}),
obstacle detection (see Section~\ref{sec:technical-obs}), and the audio
interface (see Section~\ref{sec:technical-audio}).

\begin{figure}
\includeimage{color.pdf}{8cm}{bb=0 0 474 474}
\caption{A sample scene, viewed in color. The destination object is the pink box
  in the middle. The two stacks of brown boxes are the left and right are
  obstacles.}
\label{fig:color}
\end{figure}

\subsection{Destination Detection}
\label{sec:technical-dest}

\begin{figure}
\includeimage{destination.pdf}{8cm}{bb=0 0 474 474}
\caption{A similarity view of Figure~\ref{fig:color} showing regions of the scene with a high similarity
to the color of the destination object. The green dot indicates the predicted location of the destination.}
\label{fig:destination}
\end{figure}

\subsubsection{Overview}
\label{sec:technical-dest-overview}

Guide Dog's destination detection component is responsible for identifying the
destination object in the scene, and determining it's position in space 
relative to the camera. Guide Dog was originally imagined as a system that
would be able to detect a wide variety of objects, but we were forced to
build a simpler prototype due to time constraints. As a result, the component 
is only able to detect objects of a solid color that contrast highly from the 
scene. Functionality is provided for re-calibrating the system for a new color, 
or for different lighting or exposure conditions. 

\subsubsection{Operation}
\label{sec:technical-dest-op}

The destination detection works by first measuring the color of the destination
object. This measurement can be performed on the fly, by placing the
destination object in front of the camera, and then measuring the average 
color within a center region of the image. Once the color of the destination 
object is known, we compute a similarity image where each pixel represents the 
similarity of an input pixel's color to the measured destination color. Once 
this image is obtained, we binarize the pixel values by applying a 
user-adjustable threshold. The result is a binary image like
Figure~\ref{fig:destination}. A \emph{blob detection} algorithm is then run over the 
binary image, which finds the largest connected components within the binary 
image by treating it as a 6-connected graph. The pixel location of the largest 
blob is returned as the pixel location of the destination object.

In order to determine the camera-relative position of the destination object,
we use an average of the pixels near the destination object to compute
a depth and position. This is returned as the final estimate for the
position of the destination object. 

\subsubsection{Implementation Details}
\label{sec:technical-dest-impl}

The similarity metric we use treats all pixels as vectors in 3-space. In order
to compute a final similarity value, we first normalize all pixel vectors
to unit length. We then represent the difference between two pixels by the
magnitude of their difference vector, e.g., if \emph{c} is the color of a
given pixel, and \emph{d} is the color of the destination, then the difference
\begin{math} diff = c - d \end{math} and the similarity \begin{math} 
sim = 1 - ||diff|| \end{math}. 

In order to make the system more robust to noise, we apply a Gaussian blur
filter to the similarity image with a relatively large kernel size, e.g.
10 to 15 pixels in our application. Since the similarity computation tends
to amplify the appearance of noise, and also because noise tends to
negatively effect blob detection, this added step helps significantly increase
the robustness of the component.

After computing a similarity image in this fashion, we binarize the image
using a simple user-set threshold, and pass the binary image to the OpenCV(cite)
\emph{SimpleBlobDetector}. This routine reliably detects connected components
within the image, but requires many parameters to be set properly, e.g.
min/max blob size, convexity, and separation. After detecting blobs, the 
largest blob is used to compute a centroid in 3-space using pixels belonging 
to the blob.

\subsection{Obstacle Detection}
\label{sec:technical-obs}

\begin{figure}
\includeimage{obstacle.pdf}{5cm}{bb=0 0 300 300}
\caption{A view of the scene in Figure~\ref{fig:color}. This is a top down view.
  The user's location is shown with the blue X. The obstacles are shown labeled
  with red dots and the destination is labeled with a green dot.}
\label{fig:obstacle}
\end{figure}

\subsubsection{Overview}
\label{sec:technical-obs-overview}

The second component of Guide Dog is the component to detect obstacles. This
component must detect the obstacles nearby to the user and convey the obstacles'
locations to the audio interface. Only obstacles near the user are detected in
order to prevent overwhelming the user. It is only important for the user to
know if he she is about to run into an obstacle, not if there is obstacle on the
other side of the room.

\subsubsection{Operation}
\label{sec:technical-obs-op}

The obstacle detection algorithm looks only at the depth information from the
RGB-D camera. From this, it can see objects such as the floor, obstacles and
walls. Intuitively, all of the obstacles will be above the floor, while the floor
itself is not an obstacle. This leads to a simple algorithm to detect the
obstacles: just remove the floor and everything left is an obstacle.

\subsubsection{Implementation Details}
\label{sec:technical-obs-impl}

The obstacle detection algorithm views the points in a format created by the
Point Cloud Library (PCL)\cite{pcl-website}. Each individual pixel viewed by
the camera is represented by an X coordinate, Y coordinate, depth and RGB color.
A data structure storing all of these points is called a ``point cloud'' and
represents the whole 3D environment the camera can see.

The obstacle detection algorithm first looks at the point cloud and extracts the
plane of the floor. PCL has a built-in planar segmentation library function that
detects the largest plane in view (there is an issue if the floor is not the
largest plane in view. This will be discussed in Section~\ref{sec:eval-obs}).
The plane detection algorithm gives the \emph{a}, \emph{b}, \emph{c}, and
\emph{d} coefficients of the in the following planar equation:

\begin{math}
ax + by + cz + d = 0
\end{math}

This results in an equation representing the plane of the floor. The obstacle
detection algorithm then removes all points that lie within a certain threshold
of the floor plane. The threshold helps remove noise added by the camera and is
currently set to 10 centimeters. After removing the floor, only the obstacles
are left. However, the obstacles are represented in 3D space, which is not
necessary for obstacle detection. This is because the height of an obstacle does
not matter. If there is an obstacle at any height, this needs to be
communicated to the user. This allows the obstacle detection algorithm to
convert the 3D obstacles in 2D space. To do this, the 3D obstacle coordinates
are projected onto the floor plane. Then, the floor plane is rotated so that it
is level. This provides a simple way to analyze the obstacles and calculate
distances.

Once the obstacles have been extracted and projected into a 2D space more
analysis is performed to detect their locations. The obstacles are transferred
into a black and white image: a white pixel means there is part of an obstacle
at that location, a black pixel means that location is empty. An example of this
image is shown in Figure~\ref{fig:obstacle}. This image is then
passed off to a blob detection algorithm in OpenCV\cite{opencv-website}. The
blob detection algorithm groups together contiguous areas of points. This is
helpful because it takes a bunch of individual white pixels that are all next to
each other and groups them into one single ``blob'' with a single X and Y
coordinate location. As is shown in the example image in
Figure~\ref{fig:obstacle}, each obstacle is represented as a contiguous block
of white pixels. The blob detection algorithm detects the contiguous block and
produces a single point for the blob, marked with a red dot in the example
figure.

At this point, the obstacle detection algorithm has the locations of all
obstacles. There are two more steps that must happened before the obstacles are
sent off to the audio interface. First, the obstacle detection component has not
yet had any communication with the destination detection component. That means
that the obstacle detection component has no idea where the destination object is
and that the destination component will get detected and marked as an obstacle!
In order to prevent this, the obstacle detection component gets the coordinates
of the destination object from the destination detection component. It then
compares the coordinates of each obstacle it found with the coordinates of the
destination object. If any of the obstacle coordinates are close enough to the
destination, the obstacle detection component assumes that the obstacle must
actually be the destination and removes it from its list of obstacles. Second,
the obstacle detection component is only supposed to communicate obstacles that
are near to the user, not all obstacles. To do this, the obstacle detection
component simply removes any obstacles that are too far away from the user.

At this point, the remaining obstacles are ready to be passed off to the audio
interface so they can be communicated to the user.

\subsection{Audio Interface}
\label{sec:technical-audio}

\begin{figure}
\includeimage{vsim.pdf}{6cm}{bb=0 0 371 371}
\caption{TODO: caption}
\label{fig:vsim}
\end{figure}

\subsubsection{Overview}
\label{sec:technical-audio-overview}

%(Provide the context of what your system does. A drawing might be useful here.)
Because the intended user of Guide Dog may not have normal vision, the audio 
component is meant to provide guidance to the user without the need of sight. It
uses 3D audio cues to direct the user toward the destination and warn of any 
obstacles in the user's way. This means that the audio cues appears as though
they emanate from a particular point in 3D space that corresponds to the
destination or obstacle.

\subsubsection{Operation}
\label{sec:technical-audio-op}

%(Detailed description of the different functionalities and how they work. For
%instance, describe your tracking / smoothing / recognition algorithm.)
Guide Dog's audio interface was initially imagined to be one that used voice 
commands like in a GPS navigation system. However, such a system would require
mapping and path finding within the scene, which would have been too complex
given the time constraints. Furthermore, people are very capable with finding
their own paths given a few hints about where they need to be going and what
may be in their way. Thus, Guide Dog adopted a system that gives audio hints to
the user instead.

The audio system signals the location of the destination through the use of an
audio beacon that sounds like a series of synthesized echoing beeps. It is fairly
easy to distinguish changes in tone using the beeps and it is not a sound that
quickly gets annoying to the user. The user is expected to follow the direction
of the beacon in order to get to the destination. To communicate distance to the
destination, the audio system increases or decreases the pitch of the beacon as 
the user gets closer or further away, respectively. Once the user arrives within
a few feet from the destination, the system plays a jingle to tell the user that 
he or she has arrived. 

As for obstacles, the audio system uses a ding that sounds similar
to the one that is played when a car door is left ajar. Like with the beacon, 
the goal was to find a sound that was pleasant over moderate periods of time. 
Furthermore, the ding is clearly distinguishable from the beacon. Unlike the
destination beacon, the user is expected to pick the direction of the alert and
avoid it. Only obstacles that are within about five feet of the user get alerted.
This strategy allows the user enough room to correct his or her course while 
still be helpful. If the system alerted obstacles at any distance, there may be 
too many, which would overwhelm and confuse the user, or the ones that are 
particularly far would cause the user to correct his route at times when the 
path in front of him is actually clear.

The first version of the audio system played audio cues as though they were
actually emanating from an object's real 3D position using stereo sound. However,
users could only get a fuzzy indication of where an object was located with 
respect to it being to the left or right of the user. Thus, to further clarify 
the direction of an object, the audio system continues to use stereo sound, but
divides the field of view of the camera into discrete regions that over-emphasize 
the front, left, and right directions. 

There were two designs implemented for the regions. The angle for each region
can been seen in figure \ref{fig:regions}. The first used three regions
representing left, front, and right. Since the Asus Xtion camera has a 58 degree
field of vision on the horizontal, the audio system assigns any object that
falls within the middle 20 degrees of the view as considered being in front. The
user will hear the audio cues with equal weight in both his or her ears. Any
objects that are located outside those middle 20 degrees are considered to the
immediate left or right. Thus, if an object is to the right of the middle region,
the user only hears sound in the right ear and vice versa for objects in the
left region. When finding the destination, the user can position the destination
in front and walk forward. If the user happens to not be perpendicular to the
object in real life, the destination will eventually fall to the left or right
as the user gets closer, which then forces the user to reposition. However, this
strategy appeared limited since it only communicates three directions. Thus, the
second strategy tried improve upon this.

In the second iteration on regions, the audio system divided the field of view
into five regions in an attempt to get finer directional sound. This time, the
middle 10 degrees was devoted to the front. The 15 degree arc to each side of 
the middle indicates a gradual left or right and the final 9 degrees represent
a hard left or right. The front and hard left or right regions function in the 
same manner as the first design, but the 15 degree arcs are devoted to a gradual
representation of left or right. This means that as the location of an object
sweeps through gradual right region, the weight of the sound playing in the
right ear is slowly increased while the left ear is slowed decreased. This 
strategy gives the user a better sense of how far to the right or left an object
is located compared to the first design. Both designs are functional and users
have the ability within Guide Dog to toggle between them.

\begin{figure}
\includeimage{regions.pdf}{6cm}{bb=0 0 600 450}
\caption{TODO: caption}
\label{fig:regions}
\end{figure}

\subsubsection{Implementation Details}
\label{sec:technical-audio-impl}

%(Some details on the software implementation.)
The audio system is abstracted away from the other parts of Guide Dog and
written as a class with member functions that the other systems can invoke. At 
the core of this class, know as SonicDog, is OpenAL\cite{openal-website}, which 
is an open source audio library typically used in video games to handle 3D
sound. 

SonicDog provides a thread pool and job queue for the threads. The destination
and obstacle systems fill the queue with sound sources and the threads pop them
off and become responsible for playing each source. OpenAL uses the concept of a
listener to define the coordinates of the person listening and sources to define
the objects playing a sound. The listener and sources both have locations within
the OpenAL world. SonicDog defines the listner at \emph{(0, 0, 0)} under the
\emph{(x, y, z)} coordinate system. All other sound sources are placed relative 
to the listener by defining the \emph{x} and \emph{z} coordinates of the source 
since the other systems in Guide Dog reduce the world to a 2D plane. Positive
\emph{x} denotes an object being to the right of the listener and negative 
\emph{x} as being to the left. Similarly, positive \emph{z} is in front and
negative \emph{z} is behind. Furthermore, sources have a sound buffer, which 
defines the sound to be played associated with them.

To signal destinations, the destination system registers the object with
SonicDog, which initializes the source and returns an identification number. 
The destination system them refers to this number when it wants to update the 
new location of the registered object. Each time Guide Dog updates the location,
there is a subroutine in SonicDog that calculates the angle of the object 
relative to the user and then manipulates the internal OpenAL coordinates to 
place it in the correct region. Meanwhile, the thread responsible for playing 
the beacon continuously plays the beacon every 1.5 seconds and then 
recalculates the pitch of the beacon based on the current values it has for the 
position of the destination object. SonicDog handles obstacles differently.

Because obstacles are not tracked from frame to frame, SonicDog provides an
interface to alert a group of obstacles once. Each time the obstacle detection
system runs, it can pass in a list of obstacle locations to SonicDog. The same
subroutine runs to place the obstacles into their respective regions, a new
source is created in OpenAL, and then the source is pushed onto the job queue.
From there, a worker thread pulls them off one by one and tells OpenAL to play
the source. Afterwards, the thread cleans up all the memory allocated for the
source and goes back to pull more sources off the job queue or blocks if the 
queue is empty.

The math used to place an object in a specific region is straightfoward. Given
the \emph{x} and \emph{z} of an object, SonicDog uses arctangent to calculate
the angle \begin{math}\theta\end{math} between the object and the listener and 
the Pythagorean theorem to calculate the distance \emph{d} from the object to 
the listener. If using the five-region design, then the middle is the arc 
between 85 and 95 degrees. When \begin{math}85\leq\theta\leq95\end{math}, 
SonicDog sets the internal coordinates to \emph{(0, 0, d)} so that OpenAL plays 
a sound that appears as though it is coming from in front of the user. If 
\begin{math}110\leq\theta\end{math}, SonicDog places the source at 
\emph{(-d, 0, 0)} because that region is considered the hard left. For the 15 
degree arcs representing the gradual left or right regions, SonicDog uses the 
equation \begin{math}\phi=2\theta-140\end{math} to place the source at angle 
\begin{math}\phi\end{math} in the OpenAL world, where it ranges from being 
located at 0 to 35 degrees to either side of the listener. The internal 
coordinates are then set as \begin{math}(d\cos\phi, 0, d\sin\phi)\end{math}. 
This means that there is an emphasis in the sound being played toward one side, 
but not completely in one ear.

Since this is a multi-threaded system, there are multiple locks used to 
synchronize access to the maps inside SonicDog that keep track of what objects
have been registered to it. In addition, condition variables are used to
synchronize the job queue since they allow for a thread to be blocked and not
use system resources when the queue is empty and for broadcasts to wake up
threads when the queue become non-empty.


