\section{Evaluation and Results (\textasciitilde 2 pages)}
\label{sec:eval}

This section details the authors' experiences building the Guide Dog system:
what worked, what didn't work and why.

\subsection{Destination Detection}
\label{sec:eval-dest}

Although the authors originally intended to be able to detect a variety
of complex objects as destination targets, it quickly became apparent that
the difficulty of detecting an arbitrary object had been underestimated.
As a placeholder for a more sophisticated object detector, we implemented
a detector that could find and track an object so long as it stood out
clearly from its environment. This lead to the limitation that
destination objects work best when they are of a solid color. We found
that bright magenta tended to stand out best from typical interior
environments. 

Even with the above limitation, the authors found that the orientation
of the destination object, diffuse reflection, and differences in lighting
(even between two places in the same room) were factors that could lead
to a failure to detect the destination object. To mitigate these issues,
the authors experimented with various similarity metrics such as cosine
similarity, added blob detection for the similarity image, and used a
destination object with a round surface (e.g. a coffee can) to reduce
dependence on the orientation of the object. With these improvements,
the system was able to reliably detect the destination under normal
indoor lighting conditions. 



\subsection{Obstacle Detection}
\label{sec:eval-obs}

The first difficulty the authors encountered with the obstacle detection
component was rotating the 3D view of the environment such that the floor was
level. This was an important step in the process of converting the 3D
environment to a 2D top down view (described in
Section~\ref{sec:technical-obs-impl}). The authors first attempted a series of
matrix rotations and translations, but had little luck getting this to work.
Instead, the authors discovered a projection library function in PCL. This takes
a plane and projects all the points in the environment onto this plane. The
environment if first projected onto the ground plane, which flattens all of the
obstacles into 2D, on the ground plane. Then, this result is projected on to the
X-Z axis. This way the Y component is removed, resulting in a 2D image.

Another aspect of the obstacle detection component the authors experimented with
was actually identifying the locations of the obstacles. The first approach
simply split the environment into a left and right half. The obstacle detection
component would then identify the closest point on the left side and the
closest point on the right side as obstacles. This was a simplistic approach,
but didn't work well for a few reasons. First, it did not pay attention to the
size of the obstacles. For example, the image contains small amounts of noise
that appear as small obstacles. However, the obstacle detection component should
not include these as obstacles. Second, it did not group contiguous points into
a single obstacle. This is not a problem if the obstacles are both restricted to
only one side of the environment. However, it was often the case that an
obstacle would be right in the center. So when the obstacle detection component
searched for the two closest points on the left and right it would settle upon
two points that were actually on the same obstacle.

To address these problems, the authors switched to using the blob detection
approach described in Section~\ref{sec:technical-obs-impl}. The blob detection
algorithm has parameters to set for the minimum and maximum allowable size of
the obstacles. By setting the minimum high enough, this filters out the
small areas of noise that weren't actually obstacles. Further, since the blob
detection clumps together contiguous areas there is no concern of erroneously
identifying a single physical obstacle as multiple obstacles.

One current issue the obstacle detection component has it when it fails to
detect the ground plane. The planar segmentation algorithm the obstacle
detection component uses doesn't actually detect the ground, it detects the
largest plane in view. Often times, this will be the ground. However, if the
camera is pointed too high or obstacles are blocking most of the ground it is
likely that the detected plane will not actually be the ground. The obstacle
detection component currently assumes that the largest plane in view will be the
ground plane. This results in an incorrect 2D top down view of the environment
when the largest plane detected is not actually the ground and therefore
incorrect detection of the obstacles. One way this could be handled is by
restricting the legal angles of the ground plane. If the detected ground plane
is outside of the boundaries, the obstacle detection component could remove the
points in the detected plane and then run the plane detection algorithm again
until it finds the plane within the legal boundaries of a ground plane.

The obstacle detection component also fails if the plane detection algorithm
can't find a plane at all. In this case, the obstacle detection component
currently stops and does not report any obstacles. One way to fix this would be
to use the ground plane equation from the previous camera frame. This would not
lead to the best results however, since the camera angle will naturally change
slightly between frames as the user is moving through the environment. Using an
old ground plane would likely remove some of the floor, but leave parts of the
floor that don't match up exactly with the old ground plane. This would result
in parts of the floor appearing like obstacles, which would be confusing to the
user.


\subsection{Audio Interface}
\label{sec:eval-audio}

Guide Dog's audio interface was initially imagined to be one that used voice 
commands like in a GPS navigation system. However, such a system would require
mapping and path finding within the scene, which would have been too complex
given the time constraints. Furthermore, people are very capable with finding
their own paths given a few hints about where they need to be going and what
may be in their way. Thus, Guide Dog adopted a system that gives audio hints to
the user instead.

The first version of the audio system played audio cues as though they were
actually emanating from an object's real 3D position using stereo sound. However,
users could only get a fuzzy indication of where an object was located with 
respect to it being to the left or right of the user. Thus, to further clarify 
the direction of an object, the audio system continues to use stereo sound, but
divides the field of view of the camera into discrete regions that over-emphasize 
the front, left, and right directions. 

There were two designs implemented for the regions. The angle for each region
can been seen in figure \ref{fig:regions}. The first used three regions
representing left, front, and right. Since the Asus Xtion camera has a 58 degree
field of vision on the horizontal, the audio system assigns any object that
falls within the middle 20 degrees of the view as considered being in front. The
user will hear the audio cues with equal weight in both his or her ears. Any
objects that are located outside those middle 20 degrees are considered to the
immediate left or right. Thus, if an object is to the right of the middle region,
the user only hears sound in the right ear and vice versa for objects in the
left region. When finding the destination, the user can position the destination
in front and walk forward. If the user happens to not be perpendicular to the
object in real life, the destination will eventually fall to the left or right
as the user gets closer, which then forces the user to reposition. However, this
strategy appeared limited since it only communicates three directions. Thus, the
second strategy tried improve upon this.

In the second iteration on regions, the audio system divided the field of view
into five regions in an attempt to get finer directional sound. This time, the
middle 10 degrees was devoted to the front. The 15 degree arc to each side of 
the middle indicates a gradual left or right and the final 9 degrees represent
a hard left or right. The front and hard left or right regions function in the 
same manner as the first design, but the 15 degree arcs are devoted to a gradual
representation of left or right. This means that as the location of an object
sweeps through gradual right region, the weight of the sound playing in the
right ear is slowly increased while the left ear is slowed decreased. This 
strategy gives the user a better sense of how far to the right or left an object
is located compared to the first design. Both designs are functional and users
have the ability within Guide Dog to toggle between them.

\begin{figure}
\includeimage{regions.pdf}{6cm}{bb=0 0 600 450}
\caption{TODO: caption}
\label{fig:regions}
\end{figure}

