\section{Evaluation and Results}
\label{sec:eval}

\subsection{Destination Detection}
\label{sec:eval-dest}

Although the authors originally intended to be able to detect a variety
of complex objects as destination targets, it quickly became apparent that
the difficulty of detecting an arbitrary object had been underestimated.
As a placeholder for a more sophisticated object detector, we implemented
something that could find and track an object as long as it stood out
clearly from its environment. This led to the limitation that
destination objects work best when they are of a bright, solid color. We found
that a magenta color tended to stand out best from typical interior
environments. 

Even with the above limitation, the authors found that the orientation
of the destination object, diffuse reflection, and differences in lighting
(even between two places in the same room) were factors that could lead
to a failure to detect the destination object. 

To migitage these issues, the authors experimented with different similarity
metrics, blob detection, and different colors and shapes for the destination
object. In addition to the similarity metric described in 
Section~\ref{sec:technical-dest-impl}, the authors also tried using
the cosine similarity of color vectors, but found the difference magnitude
approach to perform better in practice. The addition of blob detection
allowed the system to detect the destination object under normal variation
in room lighting. The authors also experimented with various colors and
shapes for the destination object, and found that an object with a rounded
face (such as a cylinder or ball) helped to reduce the effect of diffuse
reflections. This lead to the use of a coffee can wrapped with matte pink paper
as the final destination object. With these improvements, the system was able 
to reliably detect the destination under normal indoor lighting conditions.

\subsection{Obstacle Detection}
\label{sec:eval-obs}

The first difficulty the authors encountered with the obstacle detection
component was rotating the 3D view of the environment such that the floor was
level. This was an important step in the process of converting the 3D
environment to a 2D top down view (described in
Section~\ref{sec:technical-obs-impl}). The authors first attempted a series of
matrix rotations and translations, but had little luck getting this to work.
Instead, the authors discovered a projection library function in PCL. This takes
a plane and projects all the points in the environment onto this plane. The
environment is first projected onto the ground plane, which flattens all of the
obstacles into 2D on the ground plane. Then, this result is projected on to the
X-Z axis. This way the Y component is removed, resulting in a 2D image.

Another aspect of the obstacle detection component the authors experimented with
was actually identifying the locations of the obstacles. The first approach
simply split the environment into a left and right half. The obstacle detection
component would then identify the closest point on the left side and the
closest point on the right side as obstacles. This was a simplistic approach,
but didn't work well for a few reasons. First, it incorrectly identified noise in
the image as obstacles. For example, the images sometimes contain small amounts of noise
that appear as small obstacles. Second, it did not group contiguous points into
a single obstacle. This is not a problem if the obstacles are both restricted to
only one side of the environment. However, it was often the case that an
obstacle would be directly in the center. So when the obstacle detection component
searched for the two closest points on the left and right it would settle upon
two points that were actually part of the same obstacle.

To address these problems, the authors switched to using the blob detection
approach described in Section~\ref{sec:technical-obs-impl}. The blob detection
algorithm has parameters to set for the minimum and maximum allowable size of
the obstacles. By setting the minimum high enough, this filters out the
small areas of noise that weren't actually obstacles. Further, since the blob
detection clumps together contiguous areas, there is no concern of erroneously
identifying a single physical obstacle as multiple obstacles.

One current issue the obstacle detection component is when it fails to
detect the ground plane. The planar segmentation algorithm the obstacle
detection component uses doesn't actually detect the ground, it detects the
largest plane in view. Often times, this will be the ground. However, if the
camera is pointed too high or if obstacles are blocking most of the ground, it is
likely that the detected plane will not actually be the ground. The obstacle
detection component currently assumes that the largest plane in view will be the
ground plane. This results in an incorrect 2D top down view of the environment
when the largest plane detected is not actually the ground and therefore
incorrect detection of the obstacles. One way this could be handled is by
restricting the legal angles of the ground plane. If the detected ground plane
is outside of the boundaries, the obstacle detection component could remove the
points in the detected plane and then run the plane detection algorithm again
until it finds the plane within the legal boundaries of a ground plane.

The obstacle detection component also fails if the plane detection algorithm
can't find a plane at all. In this case, the obstacle detection component
currently stops and does not report any obstacles. One way to fix this would be
to use the ground plane equation from the previous camera frame. This would not
lead to the best results however, since the camera angle will naturally change
slightly between frames as the user is moving through the environment. Using an
old ground plane would likely remove some of the floor, but leave parts of the
floor that don't match up exactly with the old ground plane. This would result
in parts of the floor appearing like obstacles, which would be confusing to the
user.


\subsection{Audio Interface}
\label{sec:eval-audio}

\begin{figure}
\includeimage{vsim.pdf}{6cm}{bb=0 0 371 371}
\caption{A program to simulate the expected functionality of Guide Dog, which was
used to aid development of the audio system. The user is represented by the
while box, the destination the pink box, and obstacles are the green boxes. The
user could move around in the environment and interact with the audio interface.}
\label{fig:vsim}
\end{figure}

Guide Dog's audio interface was initially imagined to be one that used voice
commands like in a GPS navigation system. However, such a system would require
mapping and path finding within the scene, which would have been too complex
given the time constraints. Furthermore, people are very capable of finding
their own paths given a few hints about where they need to be going and what
may be in their way. Thus, Guide Dog adopted a system that gives audio hints to
the user instead.

To aid development of the audio system, the authors created a program to
simulate Guide Dog's functionality. It began as a terminal program with no
visual output that enabled the user to blindly traverse the landscape while
following the beacon and avoiding obstacles. Later on, a new simulator with 
visual output was created based off the tutorial by~\cite{openal-tutorial} using
OpenGL~\cite{opengl-website}. Figure~\ref{fig:vsim} gives a description of the 
\emph{vsim} program. With it, the authors could test what strategies worked and
verify that any calculations made in SonicDog were correct independent of the
other systems in Guide Dog.

\begin{figure}
\includeimage{regions.pdf}{6cm}{bb=0 0 600 450}
\caption{The division of the camera field of view into discrete regions. Each
region exaggerates its defined direction to give the user a better sense of the
location of an object. The red square represents the camera's position and the
red lines represent the extent of its 58 degree field of view. The black lines
delineate each region. On the left is the first implementation of regions and
on the right is the second.}
\label{fig:regions}
\end{figure}

The first version of the audio system played audio cues as though they were
actually emanating from an object's real 3D position using stereo sound. However,
users could only get an approximate indication of an object's location because
of the subtle changes in the audio direction. The simulation demonstrated that
it was possible to navigate using this method, but the authors wanted a system
with feedback that was more clear. Thus, to further clarify the direction of an 
object, the audio system continues to use stereo sound, but divides the field of
view of the camera into discrete regions that exaggerate the front, left, and 
right directions.

There were two designs implemented for the regions. The angles for each region 
can been seen in Figure~\ref{fig:regions}. The first used three regions 
representing left, front, and right. Since the Asus Xtion camera has a 58 degree 
horizontal field of vision~\cite{xtion-website}, the audio system considers any 
object that falls within the middle 20 degrees of the view to be in front. The 
user will hear the audio cues with equal weight in both his or her ears. Any 
objects that are located outside those middle 20 degrees are considered to be to 
the full left or right. Thus, if an object is to the right of the middle region, 
the user only hears sound in the right ear and vice versa for objects in the 
left region. When finding the destination, the user can position the destination 
in front and walk forward. If the user is actually off-center from the object in 
real life, the destination will eventually fall to the left or right as the user 
gets closer, which then forces the user to rotate and reposition in the 
middle. However, this strategy appeared limited since it only communicates 
three directions. Thus, the second strategy tried to improve upon this.

In the second iteration on regions, the audio system divided the field of view
into five regions in an attempt to get finer directional sound. This time, the
middle 10 degrees was devoted to the front. The 15 degree arcs to each side of
the middle indicates a partial left or right direction and the final 9 degrees 
represent a complete left or right. The front and complete left or right regions 
function in the same manner as the first design, but the 15 degree arcs are 
devoted to a partial representation of left or right. For instance, as the
location of an object sweeps to the right through the partial right region, the 
weight of the sound playing in the right ear slowly increases while the sound in
the left ear is slowly decreases. Compared to the first design, this strategy 
gives the user a better sense of slight changes in an object's direction. Though
it is easier with this design, the user must still listen carefully for the
shifts in stereo. But this is still better than the first version since the
complete left and right regions are there to give a clear indication of direction
should the user rotate too far away from the object. Aside from improving the
quality of directions, the authors also sought out pleasant sound cues.

Because audio is the primary means for users to interact with Guide Dog, it
needed a set of sound cues that would sound pleasant to the user and not become
annoying over time. The earliest implementation of the audio system used a
white noise tone as the beacon and sine wave tone to alert obstacles because
OpenAL could conveniently generate those tones. However, both proved to be far 
too abrasive and were discarded. The next attempts were great improvements.

In finding a good sound for the beacon, the authors wanted to emulate the sonar
sound from a submarine that one typically hears in movies. The second attempt
lead to a high pitched ping that sounded like hitting two metal pipes against
one another. It was serviceable, but as the pitch increased, it became more
unpleasant. The current version of the audio system uses a synthesized triple
beep sound that also conveys the sense of using sonar. It also has the
advantage of continuing to be pleasant sounding as the pitch increases. In
addition, it should be noted that the early white noise version of the beacon
denoted distance by increasing the volume and tempo as the user walked closer.
When the audio system started using the sonar sounds, increasing the tempo
while the sound was played became too unpleasant. Thus, pitch was chosen to
communicate distance.

For the second version of the obstacle sound, the authors wanted a sound that
could quickly grab the user's attention. For a while, the authors used a buzzing
sound, but after doing real user testing with the complete Guide Dog system,
the buzz was deemed to be too abrasive. This led to the use of a ding sound
similar to the one that is played when a person leaves a car door open. In
rapid succession, the ding is neutral enough that it does not become unpleasant
and it still sounds distinctive when played against the beacon.

Another design decision made in regards to obstacles was whether or not to alert
obstacles located to the complete left or right of the user. The user is told to
steer away from obstacles, but in practice, most obstacles that are alerted to the
left or right of the user do not obstruct his or her path if he or she continues
to walk forward. Thus, narrow walkways were often ignored. It was too confusing
to instruct the user to take heed of obstacle alerts to the side while avoid
the ones in front since the audio system used the same ding for both. Thus, the
authors tried turning off obstacles to the sides, which in practiced allowed
users to walk through narrow spaces. However, this is an area of the audio
system that needs further development.

